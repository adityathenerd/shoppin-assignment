{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_file_path = 'labeled_data.pkl'\n",
    "\n",
    "# Load the pickle file\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    labeled_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_file_path = 'filtered_df.pkl'\n",
    "\n",
    "# Load the pickle file\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    filtered_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>productDisplayName</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15970</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
       "      <td>15970.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59263</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Titan Women Silver Watch</td>\n",
       "      <td>59263.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53759</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Grey T-shirt</td>\n",
       "      <td>53759.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1855</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2011</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Inkfruit Mens Chain Reaction T-shirt</td>\n",
       "      <td>1855.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30805</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Green</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012</td>\n",
       "      <td>Ethnic</td>\n",
       "      <td>Fabindia Men Striped Green Shirt</td>\n",
       "      <td>30805.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id gender masterCategory subCategory articleType baseColour  season  \\\n",
       "0  15970    Men        Apparel     Topwear      Shirts  Navy Blue    Fall   \n",
       "2  59263  Women    Accessories     Watches     Watches     Silver  Winter   \n",
       "4  53759    Men        Apparel     Topwear     Tshirts       Grey  Summer   \n",
       "5   1855    Men        Apparel     Topwear     Tshirts       Grey  Summer   \n",
       "6  30805    Men        Apparel     Topwear      Shirts      Green  Summer   \n",
       "\n",
       "   year   usage                    productDisplayName      image  \n",
       "0  2011  Casual      Turtle Check Men Navy Blue Shirt  15970.jpg  \n",
       "2  2016  Casual              Titan Women Silver Watch  59263.jpg  \n",
       "4  2012  Casual                 Puma Men Grey T-shirt  53759.jpg  \n",
       "5  2011  Casual  Inkfruit Mens Chain Reaction T-shirt   1855.jpg  \n",
       "6  2012  Ethnic      Fabindia Men Striped Green Shirt  30805.jpg  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "ind = filtered_df.index.tolist()\n",
    "random.shuffle(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(filtered_df)\n",
    "p_train = 0.6\n",
    "p_val = 0.2\n",
    "n_train = int(p_train*n)\n",
    "n_val = int(p_val*n)\n",
    "train_ind = ind[:n_train]\n",
    "val_ind = ind[n_train:(n_train+n_val)]\n",
    "test_ind = ind[(n_train+n_val):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = []\n",
    "val_img = []\n",
    "test_img = []\n",
    "train_label = []\n",
    "val_label = []\n",
    "test_label = []\n",
    "test_ids = []\n",
    "\n",
    "for img in labeled_data:\n",
    "    if img['index'] in train_ind:\n",
    "        train_img.append(img['img'])\n",
    "        train_label.append(img['label'])\n",
    "    elif img['index'] in val_ind:\n",
    "        val_img.append(img['img'])\n",
    "        val_label.append(img['label'])\n",
    "    elif img['index'] in test_ind:\n",
    "        test_img.append(img['img'])\n",
    "        test_label.append(img['label'])\n",
    "        test_ids.append(img['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_dict({'img':train_img,'label':train_label})\n",
    "val_ds = Dataset.from_dict({'img':val_img,'label':val_label})\n",
    "test_ds = Dataset.from_dict({'img':test_img,'label':test_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "\n",
    "# 1. Load the Pretrained ResNet Model\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNetFeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        # Remove the final classification layer (fc)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # Keep all layers except the last FC layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return features.view(features.size(0), -1)  # Flatten the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66a86cde93e474aa4adc85102fe4920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/266 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--microsoft--resnet-50. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\transformers\\models\\convnext\\feature_extraction_convnext.py:28: FutureWarning: The class ConvNextFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ConvNextImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Using Hugging Face AutoFeatureExtractor\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-50\")\n",
    "\n",
    "# You can then use either one in your preprocessing function:\n",
    "def preprocess_images(examples):\n",
    "    # get batch of images\n",
    "    images = examples['img']\n",
    "    # convert to list of NumPy arrays\n",
    "    images = [np.array(image, dtype=np.uint8) for image in images]\n",
    "    \n",
    "    if isinstance(feature_extractor, transforms.Compose):\n",
    "        # For torchvision transforms\n",
    "        images = [Image.fromarray(img) for img in images]\n",
    "        images = [feature_extractor(img) for img in images]\n",
    "        examples['pixel_values'] = torch.stack(images)\n",
    "    else:\n",
    "        # For Hugging Face feature extractor\n",
    "        inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
    "        examples['pixel_values'] = inputs['pixel_values']\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleType</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tshirts</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shirts</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Casual Shoes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Watches</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sports Shoes</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kurtas</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tops</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Handbags</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heels</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wallets</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    articleType  label_num\n",
       "7       Tshirts          7\n",
       "4        Shirts          4\n",
       "0  Casual Shoes          0\n",
       "9       Watches          9\n",
       "5  Sports Shoes          5\n",
       "3        Kurtas          3\n",
       "6          Tops          6\n",
       "1      Handbags          1\n",
       "2         Heels          2\n",
       "8       Wallets          8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "top_labels = pd.DataFrame(filtered_df.groupby('articleType').size().reset_index().sort_values(0,ascending = False)[:11]['articleType'])\n",
    "top_labels_list = sorted(list(top_labels['articleType']))\n",
    "top_labels['label_num'] = top_labels['articleType'].apply(lambda x: top_labels_list.index(x))\n",
    "top_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2fd37c101f41cca26ba0abaa4684c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdb638844fd404bb48d46f69e3003cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/540 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c597bc5b37e647f1b7ed89c4d7442a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_images(examples):\n",
    "    # get batch of images\n",
    "    images = examples['img']\n",
    "    # convert to list of NumPy arrays\n",
    "    images = [np.array(image, dtype=np.uint8) for image in images]\n",
    "    \n",
    "    if isinstance(feature_extractor, transforms.Compose):\n",
    "        # For torchvision transforms\n",
    "        images = [Image.fromarray(img) for img in images]\n",
    "        processed_images = [feature_extractor(img) for img in images]\n",
    "        # Convert tensor to numpy array for storage\n",
    "        processed_images = [img.numpy() for img in processed_images]\n",
    "    else:\n",
    "        # For Hugging Face feature extractor\n",
    "        inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
    "        # Convert tensor to numpy array for storage\n",
    "        processed_images = inputs['pixel_values'].numpy()\n",
    "    \n",
    "    examples['pixel_values'] = processed_images\n",
    "    return examples\n",
    "\n",
    "# Define features\n",
    "features = Features({\n",
    "    'label': ClassLabel(names=top_labels_list),\n",
    "    'img': Array3D(dtype=\"int64\", shape=(3, 32, 32)),\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224))\n",
    "})\n",
    "\n",
    "# Process datasets\n",
    "preprocessed_train_ds = train_ds.map(\n",
    "    preprocess_images, \n",
    "    batched=True, \n",
    "    features=features,\n",
    "    batch_size=32\n",
    ")\n",
    "preprocessed_val_ds = val_ds.map(\n",
    "    preprocess_images, \n",
    "    batched=True, \n",
    "    features=features,\n",
    "    batch_size=32\n",
    ")\n",
    "preprocessed_test_ds = test_ds.map(\n",
    "    preprocess_images, \n",
    "    batched=True, \n",
    "    features=features,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        # Load pretrained ResNet\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        # Replace the final fully connected layer\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        # Get the logits from ResNet\n",
    "        logits = self.resnet(pixel_values)\n",
    "        \n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=None,\n",
    "            attentions=None,\n",
    "        )\n",
    "\n",
    "# Initialize the model\n",
    "model = CustomResNet(num_classes=len(top_labels_list))\n",
    "\n",
    "metric_name = \"accuracy\"\n",
    "\n",
    "# Training arguments remain the same\n",
    "args = TrainingArguments(\n",
    "    f\"test-clothing\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    logging_dir='logs',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15912\\3491551552.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30aca5cef8349e4b3b1d08ab914c251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2828 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.43, 'grad_norm': 18.632522583007812, 'learning_rate': 1.9929278642149933e-05, 'epoch': 0.05}\n",
      "{'loss': 2.068, 'grad_norm': 13.595705032348633, 'learning_rate': 1.985855728429986e-05, 'epoch': 0.1}\n",
      "{'loss': 1.8515, 'grad_norm': 17.159460067749023, 'learning_rate': 1.978783592644979e-05, 'epoch': 0.15}\n",
      "{'loss': 1.5488, 'grad_norm': 16.069259643554688, 'learning_rate': 1.971711456859972e-05, 'epoch': 0.2}\n",
      "{'loss': 1.3936, 'grad_norm': 11.296889305114746, 'learning_rate': 1.9646393210749647e-05, 'epoch': 0.25}\n",
      "{'loss': 1.3092, 'grad_norm': 11.984753608703613, 'learning_rate': 1.957567185289958e-05, 'epoch': 0.3}\n",
      "{'loss': 1.1375, 'grad_norm': 12.58011245727539, 'learning_rate': 1.9504950495049508e-05, 'epoch': 0.35}\n",
      "{'loss': 1.1929, 'grad_norm': 10.247995376586914, 'learning_rate': 1.9434229137199436e-05, 'epoch': 0.4}\n",
      "{'loss': 1.1302, 'grad_norm': 13.451748847961426, 'learning_rate': 1.9363507779349364e-05, 'epoch': 0.45}\n",
      "{'loss': 1.0856, 'grad_norm': 11.160205841064453, 'learning_rate': 1.9292786421499293e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0524, 'grad_norm': 17.948915481567383, 'learning_rate': 1.9222065063649225e-05, 'epoch': 0.54}\n",
      "{'loss': 0.8213, 'grad_norm': 16.622297286987305, 'learning_rate': 1.9151343705799153e-05, 'epoch': 0.59}\n",
      "{'loss': 0.8771, 'grad_norm': 8.541000366210938, 'learning_rate': 1.908062234794908e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9085, 'grad_norm': 12.750420570373535, 'learning_rate': 1.900990099009901e-05, 'epoch': 0.69}\n",
      "{'loss': 0.936, 'grad_norm': 8.17442512512207, 'learning_rate': 1.8939179632248942e-05, 'epoch': 0.74}\n",
      "{'loss': 0.7576, 'grad_norm': 10.04749584197998, 'learning_rate': 1.886845827439887e-05, 'epoch': 0.79}\n",
      "{'loss': 0.9054, 'grad_norm': 13.552844047546387, 'learning_rate': 1.87977369165488e-05, 'epoch': 0.84}\n",
      "{'loss': 0.8939, 'grad_norm': 13.008456230163574, 'learning_rate': 1.872701555869873e-05, 'epoch': 0.89}\n",
      "{'loss': 0.9522, 'grad_norm': 16.94863510131836, 'learning_rate': 1.8656294200848656e-05, 'epoch': 0.94}\n",
      "{'loss': 0.822, 'grad_norm': 12.208209037780762, 'learning_rate': 1.8585572842998587e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ef828b70184d7f8fa7ddb6f5eb038e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5873302817344666, 'eval_accuracy': 0.7759259343147278, 'eval_runtime': 32.7972, 'eval_samples_per_second': 16.465, 'eval_steps_per_second': 2.073, 'epoch': 1.0}\n",
      "{'loss': 0.6416, 'grad_norm': 11.441851615905762, 'learning_rate': 1.8514851485148516e-05, 'epoch': 1.04}\n",
      "{'loss': 0.4801, 'grad_norm': 7.6913862228393555, 'learning_rate': 1.8444130127298444e-05, 'epoch': 1.09}\n",
      "{'loss': 0.68, 'grad_norm': 13.041190147399902, 'learning_rate': 1.8373408769448376e-05, 'epoch': 1.14}\n",
      "{'loss': 0.5587, 'grad_norm': 13.647153854370117, 'learning_rate': 1.8302687411598305e-05, 'epoch': 1.19}\n",
      "{'loss': 0.6204, 'grad_norm': 12.076216697692871, 'learning_rate': 1.8231966053748233e-05, 'epoch': 1.24}\n",
      "{'loss': 0.5768, 'grad_norm': 10.498698234558105, 'learning_rate': 1.8161244695898165e-05, 'epoch': 1.29}\n",
      "{'loss': 0.6335, 'grad_norm': 4.55781364440918, 'learning_rate': 1.809052333804809e-05, 'epoch': 1.34}\n",
      "{'loss': 0.7004, 'grad_norm': 14.792625427246094, 'learning_rate': 1.8019801980198022e-05, 'epoch': 1.39}\n",
      "{'loss': 0.7102, 'grad_norm': 10.93957805633545, 'learning_rate': 1.794908062234795e-05, 'epoch': 1.44}\n",
      "{'loss': 0.546, 'grad_norm': 10.02753734588623, 'learning_rate': 1.787835926449788e-05, 'epoch': 1.49}\n",
      "{'loss': 0.5276, 'grad_norm': 13.740901947021484, 'learning_rate': 1.780763790664781e-05, 'epoch': 1.53}\n",
      "{'loss': 0.3813, 'grad_norm': 3.555530309677124, 'learning_rate': 1.773691654879774e-05, 'epoch': 1.58}\n",
      "{'loss': 0.6683, 'grad_norm': 11.710104942321777, 'learning_rate': 1.7666195190947667e-05, 'epoch': 1.63}\n",
      "{'loss': 0.493, 'grad_norm': 7.404447078704834, 'learning_rate': 1.7595473833097596e-05, 'epoch': 1.68}\n",
      "{'loss': 0.4106, 'grad_norm': 10.381779670715332, 'learning_rate': 1.7524752475247528e-05, 'epoch': 1.73}\n",
      "{'loss': 0.4966, 'grad_norm': 14.748160362243652, 'learning_rate': 1.7454031117397456e-05, 'epoch': 1.78}\n",
      "{'loss': 0.4448, 'grad_norm': 4.506373882293701, 'learning_rate': 1.7383309759547384e-05, 'epoch': 1.83}\n",
      "{'loss': 0.4548, 'grad_norm': 14.375134468078613, 'learning_rate': 1.7312588401697313e-05, 'epoch': 1.88}\n",
      "{'loss': 0.4973, 'grad_norm': 13.090934753417969, 'learning_rate': 1.724186704384724e-05, 'epoch': 1.93}\n",
      "{'loss': 0.4569, 'grad_norm': 9.06755542755127, 'learning_rate': 1.7171145685997173e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3177a72d1664e72965bf89960618dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.38418540358543396, 'eval_accuracy': 0.8759258985519409, 'eval_runtime': 30.4715, 'eval_samples_per_second': 17.721, 'eval_steps_per_second': 2.232, 'epoch': 2.0}\n",
      "{'loss': 0.4414, 'grad_norm': 4.577198505401611, 'learning_rate': 1.71004243281471e-05, 'epoch': 2.03}\n",
      "{'loss': 0.2887, 'grad_norm': 11.581165313720703, 'learning_rate': 1.702970297029703e-05, 'epoch': 2.08}\n",
      "{'loss': 0.3124, 'grad_norm': 14.301990509033203, 'learning_rate': 1.6958981612446962e-05, 'epoch': 2.13}\n",
      "{'loss': 0.3099, 'grad_norm': 4.919212341308594, 'learning_rate': 1.688826025459689e-05, 'epoch': 2.18}\n",
      "{'loss': 0.2937, 'grad_norm': 7.4494099617004395, 'learning_rate': 1.681753889674682e-05, 'epoch': 2.23}\n",
      "{'loss': 0.4454, 'grad_norm': 15.751505851745605, 'learning_rate': 1.674681753889675e-05, 'epoch': 2.28}\n",
      "{'loss': 0.3953, 'grad_norm': 17.371007919311523, 'learning_rate': 1.6676096181046676e-05, 'epoch': 2.33}\n",
      "{'loss': 0.401, 'grad_norm': 17.575441360473633, 'learning_rate': 1.6605374823196607e-05, 'epoch': 2.38}\n",
      "{'loss': 0.2838, 'grad_norm': 10.679020881652832, 'learning_rate': 1.6534653465346536e-05, 'epoch': 2.43}\n",
      "{'loss': 0.3366, 'grad_norm': 16.13880729675293, 'learning_rate': 1.6463932107496464e-05, 'epoch': 2.48}\n",
      "{'loss': 0.4246, 'grad_norm': 18.874494552612305, 'learning_rate': 1.6393210749646396e-05, 'epoch': 2.52}\n",
      "{'loss': 0.2721, 'grad_norm': 17.6267032623291, 'learning_rate': 1.6322489391796325e-05, 'epoch': 2.57}\n",
      "{'loss': 0.293, 'grad_norm': 16.642391204833984, 'learning_rate': 1.6251768033946253e-05, 'epoch': 2.62}\n",
      "{'loss': 0.2434, 'grad_norm': 2.810429811477661, 'learning_rate': 1.618104667609618e-05, 'epoch': 2.67}\n",
      "{'loss': 0.4278, 'grad_norm': 19.39680290222168, 'learning_rate': 1.6110325318246113e-05, 'epoch': 2.72}\n",
      "{'loss': 0.3266, 'grad_norm': 16.458030700683594, 'learning_rate': 1.6039603960396042e-05, 'epoch': 2.77}\n",
      "{'loss': 0.2888, 'grad_norm': 8.551750183105469, 'learning_rate': 1.596888260254597e-05, 'epoch': 2.82}\n",
      "{'loss': 0.2185, 'grad_norm': 13.216706275939941, 'learning_rate': 1.58981612446959e-05, 'epoch': 2.87}\n",
      "{'loss': 0.4709, 'grad_norm': 22.531417846679688, 'learning_rate': 1.5827439886845827e-05, 'epoch': 2.92}\n",
      "{'loss': 0.3407, 'grad_norm': 4.089059352874756, 'learning_rate': 1.575671852899576e-05, 'epoch': 2.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9d421e6a2f492ca6d7943bbb6be326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3516092002391815, 'eval_accuracy': 0.8888888955116272, 'eval_runtime': 33.8246, 'eval_samples_per_second': 15.965, 'eval_steps_per_second': 2.01, 'epoch': 3.0}\n",
      "{'loss': 0.2367, 'grad_norm': 15.738625526428223, 'learning_rate': 1.5685997171145687e-05, 'epoch': 3.02}\n",
      "{'loss': 0.1318, 'grad_norm': 8.589910507202148, 'learning_rate': 1.5615275813295616e-05, 'epoch': 3.07}\n",
      "{'loss': 0.1106, 'grad_norm': 17.403480529785156, 'learning_rate': 1.5544554455445548e-05, 'epoch': 3.12}\n",
      "{'loss': 0.1734, 'grad_norm': 7.151615619659424, 'learning_rate': 1.5473833097595473e-05, 'epoch': 3.17}\n",
      "{'loss': 0.1643, 'grad_norm': 5.025076389312744, 'learning_rate': 1.5403111739745405e-05, 'epoch': 3.22}\n",
      "{'loss': 0.2189, 'grad_norm': 9.948522567749023, 'learning_rate': 1.5332390381895333e-05, 'epoch': 3.27}\n",
      "{'loss': 0.156, 'grad_norm': 3.644188642501831, 'learning_rate': 1.526166902404526e-05, 'epoch': 3.32}\n",
      "{'loss': 0.1731, 'grad_norm': 9.135891914367676, 'learning_rate': 1.5190947666195193e-05, 'epoch': 3.37}\n",
      "{'loss': 0.2131, 'grad_norm': 21.849422454833984, 'learning_rate': 1.512022630834512e-05, 'epoch': 3.42}\n",
      "{'loss': 0.2111, 'grad_norm': 22.622968673706055, 'learning_rate': 1.504950495049505e-05, 'epoch': 3.47}\n",
      "{'loss': 0.2567, 'grad_norm': 14.501265525817871, 'learning_rate': 1.497878359264498e-05, 'epoch': 3.51}\n",
      "{'loss': 0.3331, 'grad_norm': 21.55897331237793, 'learning_rate': 1.4908062234794909e-05, 'epoch': 3.56}\n",
      "{'loss': 0.4373, 'grad_norm': 12.287729263305664, 'learning_rate': 1.4837340876944839e-05, 'epoch': 3.61}\n",
      "{'loss': 0.2478, 'grad_norm': 11.70213508605957, 'learning_rate': 1.4766619519094767e-05, 'epoch': 3.66}\n",
      "{'loss': 0.1704, 'grad_norm': 5.513599872589111, 'learning_rate': 1.4695898161244697e-05, 'epoch': 3.71}\n",
      "{'loss': 0.2251, 'grad_norm': 11.99233341217041, 'learning_rate': 1.4625176803394628e-05, 'epoch': 3.76}\n",
      "{'loss': 0.1741, 'grad_norm': 16.9173583984375, 'learning_rate': 1.4554455445544556e-05, 'epoch': 3.81}\n",
      "{'loss': 0.1544, 'grad_norm': 3.2820181846618652, 'learning_rate': 1.4483734087694486e-05, 'epoch': 3.86}\n",
      "{'loss': 0.2339, 'grad_norm': 6.906519889831543, 'learning_rate': 1.4413012729844413e-05, 'epoch': 3.91}\n",
      "{'loss': 0.1395, 'grad_norm': 14.155328750610352, 'learning_rate': 1.4342291371994343e-05, 'epoch': 3.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e05db1866f142018d517bd6c87a0ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3577110171318054, 'eval_accuracy': 0.8833333253860474, 'eval_runtime': 32.9939, 'eval_samples_per_second': 16.367, 'eval_steps_per_second': 2.061, 'epoch': 4.0}\n",
      "{'loss': 0.2026, 'grad_norm': 2.9583137035369873, 'learning_rate': 1.4271570014144273e-05, 'epoch': 4.01}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[29], line 18\u001b[0m\n",
      "\u001b[0;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n",
      "\u001b[0;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n",
      "\u001b[0;32m     10\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     14\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n",
      "\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 8. Train the Model\u001b[39;00m\n",
      "\u001b[1;32m---> 18\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 9. Evaluate the Model on Test Set\u001b[39;00m\n",
      "\u001b[0;32m     21\u001b[0m test_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(preprocessed_test_ds)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n",
      "\u001b[0;32m   2165\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n",
      "\u001b[0;32m   2166\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n",
      "\u001b[0;32m   2167\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n",
      "\u001b[0;32m   2168\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n",
      "\u001b[0;32m   2169\u001b[0m     )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "\u001b[0;32m   2517\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m   2518\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n",
      "\u001b[0;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;32m   2520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n",
      "\u001b[0;32m   2521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n",
      "\u001b[0;32m   2522\u001b[0m )\n",
      "\u001b[0;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n",
      "\u001b[1;32m-> 2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n",
      "\u001b[0;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\u001b[0;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n",
      "\u001b[0;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n",
      "\u001b[0;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n",
      "\u001b[0;32m   2530\u001b[0m ):\n",
      "\u001b[0;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n",
      "\u001b[0;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3687\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n",
      "\u001b[0;32m   3685\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;32m   3686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 3687\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   3688\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n",
      "\u001b[0;32m   3689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:1964\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1962\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1964\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n",
      "\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n",
      "\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n",
      "\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n",
      "\u001b[0;32m    580\u001b[0m     )\n",
      "\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n",
      "\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n",
      "\u001b[0;32m    583\u001b[0m )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n",
      "\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n",
      "\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n",
      "\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n",
      "\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n",
      "\u001b[0;32m    348\u001b[0m     tensors,\n",
      "\u001b[0;32m    349\u001b[0m     grad_tensors_,\n",
      "\u001b[0;32m    350\u001b[0m     retain_graph,\n",
      "\u001b[0;32m    351\u001b[0m     create_graph,\n",
      "\u001b[0;32m    352\u001b[0m     inputs,\n",
      "\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[0;32m    355\u001b[0m )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n",
      "\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n",
      "\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n",
      "\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n",
      "\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = torch.argmax(torch.tensor(logits), dim=1)\n",
    "    accuracy = (preds == torch.tensor(labels)).float().mean().item()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# 7. Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_train_ds,\n",
    "    eval_dataset=preprocessed_val_ds,\n",
    "    tokenizer=None,  # No tokenizer required for vision models\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 8. Train the Model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252d4f402f574640be8e880245e2caa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4226503372192383, 'eval_accuracy': 0.8625954389572144, 'eval_runtime': 25.0073, 'eval_samples_per_second': 20.954, 'eval_steps_per_second': 2.639, 'epoch': 4.04}\n",
      "Test Results: {'eval_loss': 0.4226503372192383, 'eval_accuracy': 0.8625954389572144, 'eval_runtime': 25.0073, 'eval_samples_per_second': 20.954, 'eval_steps_per_second': 2.639, 'epoch': 4.044554455445544}\n"
     ]
    }
   ],
   "source": [
    "# 9. Evaluate the Model on Test Set\n",
    "test_results = trainer.evaluate(preprocessed_test_ds)\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d1b2293bb746c4918b0071dde8f219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = trainer.predict(preprocessed_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.4226503372192383, 'test_accuracy': 0.8625954389572144, 'test_runtime': 22.758, 'test_samples_per_second': 23.025, 'test_steps_per_second': 2.9}\n"
     ]
    }
   ],
   "source": [
    "print(outputs.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\ASUS\\\\Desktop\\\\shoppin-assignment\\\\resnet-model-trained\\\\preprocessor_config.json']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), r\"C:\\Users\\ASUS\\Desktop\\shoppin-assignment\\resnet-model-trained\\model.pth\")\n",
    "\n",
    "# Save the feature extractor\n",
    "feature_extractor.save_pretrained(r\"C:\\Users\\ASUS\\Desktop\\shoppin-assignment\\resnet-model-trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15912\\2217045441.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"C:\\Users\\ASUS\\Desktop\\shoppin-assignment\\resnet-model-trained\\model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomResNet(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the model first\n",
    "model = CustomResNet(num_classes=10)  # or however many labels you have\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\ASUS\\Desktop\\shoppin-assignment\\resnet-model-trained\\model.pth\"))\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Extracting embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 524/524 [00:32<00:00, 16.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of embeddings: 524\n",
      "Embedding dimension: 2048\n",
      "\n",
      "Sample embedding for ID 15970:\n",
      "Shape: (2048,)\n",
      "First few values: [0.1663723  0.33546552 0.03094349 0.9092974  1.0120952 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def modified_forward(self, pixel_values):\n",
    "    # Get features from the layer before the final classification layer\n",
    "    # Remove the final classification layer temporarily\n",
    "    original_fc = self.resnet.fc\n",
    "    self.resnet.fc = nn.Identity()\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        embeddings = self.resnet(pixel_values)\n",
    "    \n",
    "    # Restore the original classification layer\n",
    "    self.resnet.fc = original_fc\n",
    "    return embeddings\n",
    "\n",
    "# Replace model's forward method temporarily\n",
    "model.forward = modified_forward.__get__(model)\n",
    "\n",
    "# Run inference to get the embeddings\n",
    "embeddings = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "for batch in tqdm(preprocessed_test_ds, desc=\"Extracting embeddings\"):\n",
    "    try:\n",
    "        # Get the preprocessed pixel values\n",
    "        inputs = batch['pixel_values']\n",
    "        # Convert to tensor and ensure correct shape\n",
    "        inputs = torch.tensor(inputs).float().to(device)  # Convert to float tensor\n",
    "        if inputs.dim() == 3:\n",
    "            inputs = inputs.unsqueeze(0)  # Add batch dimension if needed\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = model(pixel_values=inputs)\n",
    "            # Move to CPU and convert to numpy\n",
    "            embedding = embedding.cpu().numpy()\n",
    "            embeddings.append(embedding)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Concatenate all embeddings\n",
    "embeddings = np.concatenate(embeddings, axis=0)\n",
    "\n",
    "# Create vectors dictionary\n",
    "vectors = {test_ids[i]: embeddings[i] for i in range(len(test_ids))}\n",
    "\n",
    "# Verify the embeddings\n",
    "print(f\"\\nNumber of embeddings: {len(vectors)}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1] if len(embeddings.shape) > 1 else 'Unknown'}\")\n",
    "\n",
    "# Optional: Print a sample embedding to verify the values\n",
    "sample_id = next(iter(vectors.keys()))\n",
    "print(f\"\\nSample embedding for ID {sample_id}:\")\n",
    "print(f\"Shape: {vectors[sample_id].shape}\")\n",
    "print(f\"First few values: {vectors[sample_id][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(vectors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_similarity_matrix(embeddings):\n",
    "    \"\"\"\n",
    "    Compute a pairwise cosine similarity matrix from the embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = np.array(embeddings)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    return similarity_matrix\n",
    "\n",
    "def evaluate_similarity(vectors, labels, top_k=5):\n",
    "    \"\"\"\n",
    "    Evaluate the model using embeddings for image similarity.\n",
    "    \n",
    "    Args:\n",
    "        vectors (dict): A dictionary of image IDs and their corresponding embeddings.\n",
    "        labels (list): Ground truth labels for the images.\n",
    "        top_k (int): Number of top results to consider for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Convert vectors to matrix and maintain IDs\n",
    "    ids = list(vectors.keys())  # Convert keys to list to ensure consistent ordering\n",
    "    embeddings = [vectors[key] for key in ids]  # Use ids list directly\n",
    "    \n",
    "    # Convert embeddings to 2D array if needed\n",
    "    embeddings = np.array(embeddings)\n",
    "    if len(embeddings.shape) == 3:\n",
    "        embeddings = embeddings.squeeze(1)  # Remove extra dimension if present\n",
    "        \n",
    "    similarity_matrix = compute_similarity_matrix(embeddings)\n",
    "\n",
    "    # Evaluation metrics\n",
    "    correct_top_k = 0\n",
    "    total_queries = len(labels)\n",
    "    \n",
    "    # Convert labels to numpy array for easier indexing\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    for idx in tqdm(range(len(ids)), desc=\"Evaluating Precision@K\"):\n",
    "        query_label = labels[idx]\n",
    "        \n",
    "        # Get similarities for current query\n",
    "        similarities = similarity_matrix[idx].copy()  # Make a copy to avoid modifying original\n",
    "        similarities[idx] = -np.inf  # Exclude self-match\n",
    "        \n",
    "        # Get top-k most similar image indices\n",
    "        top_k_indices = np.argsort(similarities)[-top_k:]\n",
    "        \n",
    "        # Check if any of the top-k have the same label\n",
    "        top_k_labels = labels[top_k_indices]\n",
    "        if query_label in top_k_labels:\n",
    "            correct_top_k += 1\n",
    "\n",
    "    precision_at_k = correct_top_k / total_queries\n",
    "\n",
    "    return {\n",
    "        \"precision@K\": precision_at_k,\n",
    "        \"total_queries\": total_queries,\n",
    "        \"correct_top_k\": correct_top_k\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Precision@K: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 524/524 [00:00<00:00, 42307.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topk=1 Evaluation Results: {'precision@K': 0.851145038167939, 'total_queries': 524, 'correct_top_k': 446}\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Precision@K: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 524/524 [00:00<00:00, 53711.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topk=5 Evaluation Results: {'precision@K': 0.9713740458015268, 'total_queries': 524, 'correct_top_k': 509}\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Precision@K: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 524/524 [00:00<00:00, 36163.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topk=10 Evaluation Results: {'precision@K': 0.9866412213740458, 'total_queries': 524, 'correct_top_k': 517}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "results = evaluate_similarity(vectors, outputs.label_ids, top_k=1)\n",
    "print(\"Topk=1 Evaluation Results:\", results)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "results = evaluate_similarity(vectors, outputs.label_ids, top_k=5)\n",
    "print(\"Topk=5 Evaluation Results:\", results)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "results = evaluate_similarity(vectors, outputs.label_ids, top_k=10)\n",
    "print(\"Topk=10 Evaluation Results:\", results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
