{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_file_path = 'labeled_data.pkl'\n",
    "\n",
    "# Load the pickle file\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    labeled_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_file_path = 'filtered_df.pkl'\n",
    "\n",
    "# Load the pickle file\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    filtered_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>productDisplayName</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15970</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
       "      <td>15970.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59263</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Titan Women Silver Watch</td>\n",
       "      <td>59263.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53759</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Grey T-shirt</td>\n",
       "      <td>53759.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1855</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2011</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Inkfruit Mens Chain Reaction T-shirt</td>\n",
       "      <td>1855.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30805</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Green</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012</td>\n",
       "      <td>Ethnic</td>\n",
       "      <td>Fabindia Men Striped Green Shirt</td>\n",
       "      <td>30805.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id gender masterCategory subCategory articleType baseColour  season  \\\n",
       "0  15970    Men        Apparel     Topwear      Shirts  Navy Blue    Fall   \n",
       "2  59263  Women    Accessories     Watches     Watches     Silver  Winter   \n",
       "4  53759    Men        Apparel     Topwear     Tshirts       Grey  Summer   \n",
       "5   1855    Men        Apparel     Topwear     Tshirts       Grey  Summer   \n",
       "6  30805    Men        Apparel     Topwear      Shirts      Green  Summer   \n",
       "\n",
       "   year   usage                    productDisplayName      image  \n",
       "0  2011  Casual      Turtle Check Men Navy Blue Shirt  15970.jpg  \n",
       "2  2016  Casual              Titan Women Silver Watch  59263.jpg  \n",
       "4  2012  Casual                 Puma Men Grey T-shirt  53759.jpg  \n",
       "5  2011  Casual  Inkfruit Mens Chain Reaction T-shirt   1855.jpg  \n",
       "6  2012  Ethnic      Fabindia Men Striped Green Shirt  30805.jpg  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "ind = filtered_df.index.tolist()\n",
    "random.shuffle(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(filtered_df)\n",
    "p_train = 0.6\n",
    "p_val = 0.2\n",
    "n_train = int(p_train*n)\n",
    "n_val = int(p_val*n)\n",
    "train_ind = ind[:n_train]\n",
    "val_ind = ind[n_train:(n_train+n_val)]\n",
    "test_ind = ind[(n_train+n_val):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = []\n",
    "val_img = []\n",
    "test_img = []\n",
    "train_label = []\n",
    "val_label = []\n",
    "test_label = []\n",
    "test_ids = []\n",
    "\n",
    "for img in labeled_data:\n",
    "    if img['index'] in train_ind:\n",
    "        train_img.append(img['img'])\n",
    "        train_label.append(img['label'])\n",
    "    elif img['index'] in val_ind:\n",
    "        val_img.append(img['img'])\n",
    "        val_label.append(img['label'])\n",
    "    elif img['index'] in test_ind:\n",
    "        test_img.append(img['img'])\n",
    "        test_label.append(img['label'])\n",
    "        test_ids.append(img['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_dict({'img':train_img,'label':train_label})\n",
    "val_ds = Dataset.from_dict({'img':val_img,'label':val_label})\n",
    "test_ds = Dataset.from_dict({'img':test_img,'label':test_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(examples):\n",
    "    # get batch of images\n",
    "    images = examples['img']\n",
    "    # convert to list of NumPy arrays of shape (C, H, W)\n",
    "    images = [np.array(image, dtype=np.uint8) for image in images]\n",
    "    images = [np.moveaxis(image, source=-1, destination=0) for image in images]\n",
    "    # preprocess and add pixel_values\n",
    "    inputs = feature_extractor(images=images)\n",
    "    examples['pixel_values'] = inputs['pixel_values']\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleType</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tshirts</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shirts</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Casual Shoes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Watches</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sports Shoes</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kurtas</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tops</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Handbags</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heels</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wallets</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    articleType  label_num\n",
       "7       Tshirts          7\n",
       "4        Shirts          4\n",
       "0  Casual Shoes          0\n",
       "9       Watches          9\n",
       "5  Sports Shoes          5\n",
       "3        Kurtas          3\n",
       "6          Tops          6\n",
       "1      Handbags          1\n",
       "2         Heels          2\n",
       "8       Wallets          8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_labels = pd.DataFrame(filtered_df.groupby('articleType').size().reset_index().sort_values(0,ascending = False)[:11]['articleType'])\n",
    "top_labels_list = sorted(list(top_labels['articleType']))\n",
    "top_labels['label_num'] = top_labels['articleType'].apply(lambda x: top_labels_list.index(x))\n",
    "top_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c17e5b253a4f70ac8d7b4561548fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2426ea2922a64d27a1651d64fa69b33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/535 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d915819127b474c877da65809f609f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Features, ClassLabel, Array3D\n",
    "\n",
    "# we need to define the features ourselves as both the img and pixel_values have a 3D shape \n",
    "features = Features({\n",
    "    'label': ClassLabel(names = top_labels_list),\n",
    "    'img': Array3D(dtype=\"int64\", shape=(3,32,32)),\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "})\n",
    "\n",
    "preprocessed_train_ds = train_ds.map(preprocess_images, batched=True, features=features)\n",
    "preprocessed_val_ds = val_ds.map(preprocess_images, batched=True, features=features)\n",
    "preprocessed_test_ds = test_ds.map(preprocess_images, batched=True, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'img', 'pixel_values'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_train_ds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting training dataset...\n",
      "Converting validation dataset...\n",
      "Converting test dataset...\n",
      "Image batch shape: (8, 224, 224, 3)\n",
      "Labels batch shape: (8, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def to_tf_dataset(hf_dataset, batch_size=8):\n",
    "    \"\"\"Convert HuggingFace dataset to TensorFlow dataset\n",
    "    \n",
    "    Args:\n",
    "        hf_dataset: HuggingFace dataset to convert\n",
    "        batch_size: Batch size for the dataset\n",
    "    \"\"\"\n",
    "    def generator():\n",
    "        for example in hf_dataset:\n",
    "            yield (\n",
    "                example['pixel_values'],  # Image data\n",
    "                example['label']  # Label\n",
    "            )\n",
    "    \n",
    "    # Create TensorFlow dataset\n",
    "    tf_dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(3, 224, 224), dtype=tf.float32),  # Image shape\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int64)  # Label shape\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Transform the dataset\n",
    "    tf_dataset = tf_dataset.map(\n",
    "        lambda x, y: (\n",
    "            tf.transpose(x, [1, 2, 0]),  # Transpose from (C,H,W) to (H,W,C)\n",
    "            tf.one_hot(y, depth=10)  # Convert label to one-hot\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Batch and prefetch\n",
    "    return tf_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Convert datasets\n",
    "print(\"Converting training dataset...\")\n",
    "train_tf_ds = to_tf_dataset(preprocessed_train_ds)\n",
    "print(\"Converting validation dataset...\")\n",
    "val_tf_ds = to_tf_dataset(preprocessed_val_ds)\n",
    "print(\"Converting test dataset...\")\n",
    "test_tf_ds = to_tf_dataset(preprocessed_test_ds)\n",
    "\n",
    "# Verify the dataset format\n",
    "for images, labels in train_tf_ds.take(1):\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Labels batch shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "\n",
    "# Load ResNet50 without the top layer\n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model's layers (optional)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom classification layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "output = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "# Define the final model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 306ms/step - accuracy: 0.8099 - loss: 0.4918 - val_accuracy: 0.7377 - val_loss: 0.8021\n",
      "Epoch 2/4\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 421ms/step - accuracy: 0.8156 - loss: 0.4676 - val_accuracy: 0.7358 - val_loss: 0.8060\n",
      "Epoch 3/4\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 363ms/step - accuracy: 0.8188 - loss: 0.4636 - val_accuracy: 0.7321 - val_loss: 0.8108\n",
      "Epoch 4/4\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 382ms/step - accuracy: 0.8228 - loss: 0.4552 - val_accuracy: 0.7321 - val_loss: 0.8079\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_tf_ds,\n",
    "    validation_data=val_tf_ds,\n",
    "    epochs=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 324ms/step - accuracy: 0.7355 - loss: 0.7991\n",
      "Test Accuracy: 0.7481\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_tf_ds)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mASUS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mshoppin-assignment\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mresnet50-trained\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(r'C:\\Users\\ASUS\\Desktop\\shoppin-assignment\\resnet50-trained\\model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(r'C:\\Users\\ASUS\\Desktop\\shoppin-assignment\\resnet50-trained\\model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings and predictions...\n",
      "\n",
      "Available layers:\n",
      "0: input_layer_1\n",
      "1: conv1_pad\n",
      "2: conv1_conv\n",
      "3: conv1_bn\n",
      "4: conv1_relu\n",
      "5: pool1_pad\n",
      "6: pool1_pool\n",
      "7: conv2_block1_1_conv\n",
      "8: conv2_block1_1_bn\n",
      "9: conv2_block1_1_relu\n",
      "10: conv2_block1_2_conv\n",
      "11: conv2_block1_2_bn\n",
      "12: conv2_block1_2_relu\n",
      "13: conv2_block1_0_conv\n",
      "14: conv2_block1_3_conv\n",
      "15: conv2_block1_0_bn\n",
      "16: conv2_block1_3_bn\n",
      "17: conv2_block1_add\n",
      "18: conv2_block1_out\n",
      "19: conv2_block2_1_conv\n",
      "20: conv2_block2_1_bn\n",
      "21: conv2_block2_1_relu\n",
      "22: conv2_block2_2_conv\n",
      "23: conv2_block2_2_bn\n",
      "24: conv2_block2_2_relu\n",
      "25: conv2_block2_3_conv\n",
      "26: conv2_block2_3_bn\n",
      "27: conv2_block2_add\n",
      "28: conv2_block2_out\n",
      "29: conv2_block3_1_conv\n",
      "30: conv2_block3_1_bn\n",
      "31: conv2_block3_1_relu\n",
      "32: conv2_block3_2_conv\n",
      "33: conv2_block3_2_bn\n",
      "34: conv2_block3_2_relu\n",
      "35: conv2_block3_3_conv\n",
      "36: conv2_block3_3_bn\n",
      "37: conv2_block3_add\n",
      "38: conv2_block3_out\n",
      "39: conv3_block1_1_conv\n",
      "40: conv3_block1_1_bn\n",
      "41: conv3_block1_1_relu\n",
      "42: conv3_block1_2_conv\n",
      "43: conv3_block1_2_bn\n",
      "44: conv3_block1_2_relu\n",
      "45: conv3_block1_0_conv\n",
      "46: conv3_block1_3_conv\n",
      "47: conv3_block1_0_bn\n",
      "48: conv3_block1_3_bn\n",
      "49: conv3_block1_add\n",
      "50: conv3_block1_out\n",
      "51: conv3_block2_1_conv\n",
      "52: conv3_block2_1_bn\n",
      "53: conv3_block2_1_relu\n",
      "54: conv3_block2_2_conv\n",
      "55: conv3_block2_2_bn\n",
      "56: conv3_block2_2_relu\n",
      "57: conv3_block2_3_conv\n",
      "58: conv3_block2_3_bn\n",
      "59: conv3_block2_add\n",
      "60: conv3_block2_out\n",
      "61: conv3_block3_1_conv\n",
      "62: conv3_block3_1_bn\n",
      "63: conv3_block3_1_relu\n",
      "64: conv3_block3_2_conv\n",
      "65: conv3_block3_2_bn\n",
      "66: conv3_block3_2_relu\n",
      "67: conv3_block3_3_conv\n",
      "68: conv3_block3_3_bn\n",
      "69: conv3_block3_add\n",
      "70: conv3_block3_out\n",
      "71: conv3_block4_1_conv\n",
      "72: conv3_block4_1_bn\n",
      "73: conv3_block4_1_relu\n",
      "74: conv3_block4_2_conv\n",
      "75: conv3_block4_2_bn\n",
      "76: conv3_block4_2_relu\n",
      "77: conv3_block4_3_conv\n",
      "78: conv3_block4_3_bn\n",
      "79: conv3_block4_add\n",
      "80: conv3_block4_out\n",
      "81: conv4_block1_1_conv\n",
      "82: conv4_block1_1_bn\n",
      "83: conv4_block1_1_relu\n",
      "84: conv4_block1_2_conv\n",
      "85: conv4_block1_2_bn\n",
      "86: conv4_block1_2_relu\n",
      "87: conv4_block1_0_conv\n",
      "88: conv4_block1_3_conv\n",
      "89: conv4_block1_0_bn\n",
      "90: conv4_block1_3_bn\n",
      "91: conv4_block1_add\n",
      "92: conv4_block1_out\n",
      "93: conv4_block2_1_conv\n",
      "94: conv4_block2_1_bn\n",
      "95: conv4_block2_1_relu\n",
      "96: conv4_block2_2_conv\n",
      "97: conv4_block2_2_bn\n",
      "98: conv4_block2_2_relu\n",
      "99: conv4_block2_3_conv\n",
      "100: conv4_block2_3_bn\n",
      "101: conv4_block2_add\n",
      "102: conv4_block2_out\n",
      "103: conv4_block3_1_conv\n",
      "104: conv4_block3_1_bn\n",
      "105: conv4_block3_1_relu\n",
      "106: conv4_block3_2_conv\n",
      "107: conv4_block3_2_bn\n",
      "108: conv4_block3_2_relu\n",
      "109: conv4_block3_3_conv\n",
      "110: conv4_block3_3_bn\n",
      "111: conv4_block3_add\n",
      "112: conv4_block3_out\n",
      "113: conv4_block4_1_conv\n",
      "114: conv4_block4_1_bn\n",
      "115: conv4_block4_1_relu\n",
      "116: conv4_block4_2_conv\n",
      "117: conv4_block4_2_bn\n",
      "118: conv4_block4_2_relu\n",
      "119: conv4_block4_3_conv\n",
      "120: conv4_block4_3_bn\n",
      "121: conv4_block4_add\n",
      "122: conv4_block4_out\n",
      "123: conv4_block5_1_conv\n",
      "124: conv4_block5_1_bn\n",
      "125: conv4_block5_1_relu\n",
      "126: conv4_block5_2_conv\n",
      "127: conv4_block5_2_bn\n",
      "128: conv4_block5_2_relu\n",
      "129: conv4_block5_3_conv\n",
      "130: conv4_block5_3_bn\n",
      "131: conv4_block5_add\n",
      "132: conv4_block5_out\n",
      "133: conv4_block6_1_conv\n",
      "134: conv4_block6_1_bn\n",
      "135: conv4_block6_1_relu\n",
      "136: conv4_block6_2_conv\n",
      "137: conv4_block6_2_bn\n",
      "138: conv4_block6_2_relu\n",
      "139: conv4_block6_3_conv\n",
      "140: conv4_block6_3_bn\n",
      "141: conv4_block6_add\n",
      "142: conv4_block6_out\n",
      "143: conv5_block1_1_conv\n",
      "144: conv5_block1_1_bn\n",
      "145: conv5_block1_1_relu\n",
      "146: conv5_block1_2_conv\n",
      "147: conv5_block1_2_bn\n",
      "148: conv5_block1_2_relu\n",
      "149: conv5_block1_0_conv\n",
      "150: conv5_block1_3_conv\n",
      "151: conv5_block1_0_bn\n",
      "152: conv5_block1_3_bn\n",
      "153: conv5_block1_add\n",
      "154: conv5_block1_out\n",
      "155: conv5_block2_1_conv\n",
      "156: conv5_block2_1_bn\n",
      "157: conv5_block2_1_relu\n",
      "158: conv5_block2_2_conv\n",
      "159: conv5_block2_2_bn\n",
      "160: conv5_block2_2_relu\n",
      "161: conv5_block2_3_conv\n",
      "162: conv5_block2_3_bn\n",
      "163: conv5_block2_add\n",
      "164: conv5_block2_out\n",
      "165: conv5_block3_1_conv\n",
      "166: conv5_block3_1_bn\n",
      "167: conv5_block3_1_relu\n",
      "168: conv5_block3_2_conv\n",
      "169: conv5_block3_2_bn\n",
      "170: conv5_block3_2_relu\n",
      "171: conv5_block3_3_conv\n",
      "172: conv5_block3_3_bn\n",
      "173: conv5_block3_add\n",
      "174: conv5_block3_out\n",
      "175: global_average_pooling2d_1\n",
      "176: dense_2\n",
      "177: dense_3\n",
      "\n",
      "Using layer 'dense_2' for embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 65it [00:28,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final shapes:\n",
      "Embeddings shape: (520, 256)\n",
      "Predictions shape: (520, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_with_resnet50(model, test_dataset):\n",
    "    \"\"\"\n",
    "    Perform predictions using ResNet50.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    predictions = []\n",
    "    \n",
    "    # Print all layer names to help identify the correct layer\n",
    "    print(\"\\nAvailable layers:\")\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        print(f\"{i}: {layer.name}\")\n",
    "    \n",
    "    # Create embedding model using the second-to-last layer\n",
    "    # You can adjust the index based on the printed layer names\n",
    "    embedding_layer = model.layers[-2]  # Get second-to-last layer\n",
    "    print(f\"\\nUsing layer '{embedding_layer.name}' for embeddings\")\n",
    "    \n",
    "    embedding_model = tf.keras.Model(\n",
    "        inputs=model.input,\n",
    "        outputs=embedding_layer.output\n",
    "    )\n",
    "    \n",
    "    # Iterate through the dataset\n",
    "    for images, labels in tqdm(test_dataset, desc=\"Generating predictions\"):\n",
    "        try:\n",
    "            # Get predictions and embeddings\n",
    "            preds = model.predict(images, verbose=0)\n",
    "            embs = embedding_model.predict(images, verbose=0)\n",
    "            \n",
    "            # Store results\n",
    "            embeddings.append(embs)\n",
    "            predictions.append(preds)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    \n",
    "    print(f\"\\nFinal shapes:\")\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    \n",
    "    return embeddings, predictions\n",
    "\n",
    "# Use the function with the TensorFlow dataset\n",
    "print(\"Generating embeddings and predictions...\")\n",
    "embeddings, predictions = predict_with_resnet50(model, test_tf_ds)\n",
    "\n",
    "# Create vectors dictionary with test IDs\n",
    "vectors = {test_ids[i]: embeddings[i] for i in range(len(test_ids))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_similarity_matrix(embeddings):\n",
    "    \"\"\"\n",
    "    Compute a pairwise cosine similarity matrix from the embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = np.array(embeddings)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    return similarity_matrix\n",
    "\n",
    "def evaluate_similarity(vectors, labels, top_k=5):\n",
    "    \"\"\"\n",
    "    Evaluate the model using embeddings for image similarity.\n",
    "    \n",
    "    Args:\n",
    "        vectors (dict): A dictionary of image IDs and their corresponding embeddings.\n",
    "        labels (list): Ground truth labels for the images.\n",
    "        top_k (int): Number of top results to consider for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Convert vectors to matrix and maintain IDs\n",
    "    ids = list(vectors.keys())  # Convert keys to list to ensure consistent ordering\n",
    "    embeddings = [vectors[key] for key in ids]  # Use ids list directly\n",
    "    \n",
    "    # Convert embeddings to 2D array if needed\n",
    "    embeddings = np.array(embeddings)\n",
    "    if len(embeddings.shape) == 3:\n",
    "        embeddings = embeddings.squeeze(1)  # Remove extra dimension if present\n",
    "        \n",
    "    similarity_matrix = compute_similarity_matrix(embeddings)\n",
    "\n",
    "    # Evaluation metrics\n",
    "    correct_top_k = 0\n",
    "    total_queries = len(labels)\n",
    "    \n",
    "    # Convert labels to numpy array for easier indexing\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    for idx in tqdm(range(len(ids)), desc=\"Evaluating Precision@K\"):\n",
    "        query_label = labels[idx]\n",
    "        \n",
    "        # Get similarities for current query\n",
    "        similarities = similarity_matrix[idx].copy()  # Make a copy to avoid modifying original\n",
    "        similarities[idx] = -np.inf  # Exclude self-match\n",
    "        \n",
    "        # Get top-k most similar image indices\n",
    "        top_k_indices = np.argsort(similarities)[-top_k:]\n",
    "        \n",
    "        # Check if any of the top-k have the same label\n",
    "        top_k_labels = labels[top_k_indices]\n",
    "        if query_label in top_k_labels:\n",
    "            correct_top_k += 1\n",
    "\n",
    "    precision_at_k = correct_top_k / total_queries\n",
    "\n",
    "    return {\n",
    "        \"precision@K\": precision_at_k,\n",
    "        \"total_queries\": total_queries,\n",
    "        \"correct_top_k\": correct_top_k\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Precision@K: 100%|██████████| 520/520 [00:00<00:00, 29409.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topk=1 Evaluation Results: {'precision@K': 0.7653846153846153, 'total_queries': 520, 'correct_top_k': 398}\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Precision@K: 100%|██████████| 520/520 [00:00<00:00, 44097.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topk=5 Evaluation Results: {'precision@K': 0.9442307692307692, 'total_queries': 520, 'correct_top_k': 491}\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Precision@K: 100%|██████████| 520/520 [00:00<00:00, 121249.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topk=10 Evaluation Results: {'precision@K': 0.9769230769230769, 'total_queries': 520, 'correct_top_k': 508}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "results = evaluate_similarity(vectors, test_label, top_k=1)\n",
    "print(\"Topk=1 Evaluation Results:\", results)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "results = evaluate_similarity(vectors, test_label, top_k=5)\n",
    "print(\"Topk=5 Evaluation Results:\", results)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "results = evaluate_similarity(vectors, test_label, top_k=10)\n",
    "print(\"Topk=10 Evaluation Results:\", results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
