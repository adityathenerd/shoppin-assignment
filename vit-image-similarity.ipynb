{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_file_path = 'labeled_data.pkl'\n",
    "\n",
    "# Load the pickle file\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    labeled_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_file_path = 'filtered_df.pkl'\n",
    "\n",
    "# Load the pickle file\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    filtered_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>productDisplayName</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15970</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
       "      <td>15970.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59263</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Titan Women Silver Watch</td>\n",
       "      <td>59263.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53759</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Grey T-shirt</td>\n",
       "      <td>53759.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1855</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2011</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Inkfruit Mens Chain Reaction T-shirt</td>\n",
       "      <td>1855.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30805</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Green</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012</td>\n",
       "      <td>Ethnic</td>\n",
       "      <td>Fabindia Men Striped Green Shirt</td>\n",
       "      <td>30805.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id gender masterCategory subCategory articleType baseColour  season  \\\n",
       "0  15970    Men        Apparel     Topwear      Shirts  Navy Blue    Fall   \n",
       "2  59263  Women    Accessories     Watches     Watches     Silver  Winter   \n",
       "4  53759    Men        Apparel     Topwear     Tshirts       Grey  Summer   \n",
       "5   1855    Men        Apparel     Topwear     Tshirts       Grey  Summer   \n",
       "6  30805    Men        Apparel     Topwear      Shirts      Green  Summer   \n",
       "\n",
       "   year   usage                    productDisplayName      image  \n",
       "0  2011  Casual      Turtle Check Men Navy Blue Shirt  15970.jpg  \n",
       "2  2016  Casual              Titan Women Silver Watch  59263.jpg  \n",
       "4  2012  Casual                 Puma Men Grey T-shirt  53759.jpg  \n",
       "5  2011  Casual  Inkfruit Mens Chain Reaction T-shirt   1855.jpg  \n",
       "6  2012  Ethnic      Fabindia Men Striped Green Shirt  30805.jpg  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "ind = filtered_df.index.tolist()\n",
    "random.shuffle(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(filtered_df)\n",
    "p_train = 0.6\n",
    "p_val = 0.2\n",
    "n_train = int(p_train*n)\n",
    "n_val = int(p_val*n)\n",
    "train_ind = ind[:n_train]\n",
    "val_ind = ind[n_train:(n_train+n_val)]\n",
    "test_ind = ind[(n_train+n_val):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = []\n",
    "val_img = []\n",
    "test_img = []\n",
    "train_label = []\n",
    "val_label = []\n",
    "test_label = []\n",
    "test_ids = []\n",
    "\n",
    "for img in labeled_data:\n",
    "    if img['index'] in train_ind:\n",
    "        train_img.append(img['img'])\n",
    "        train_label.append(img['label'])\n",
    "    elif img['index'] in val_ind:\n",
    "        val_img.append(img['img'])\n",
    "        val_label.append(img['label'])\n",
    "    elif img['index'] in test_ind:\n",
    "        test_img.append(img['img'])\n",
    "        test_label.append(img['label'])\n",
    "        test_ids.append(img['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_dict({'img':train_img,'label':train_label})\n",
    "val_ds = Dataset.from_dict({'img':val_img,'label':val_label})\n",
    "test_ds = Dataset.from_dict({'img':test_img,'label':test_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(examples):\n",
    "    # get batch of images\n",
    "    images = examples['img']\n",
    "    # convert to list of NumPy arrays of shape (C, H, W)\n",
    "    images = [np.array(image, dtype=np.uint8) for image in images]\n",
    "    images = [np.moveaxis(image, source=-1, destination=0) for image in images]\n",
    "    # preprocess and add pixel_values\n",
    "    inputs = feature_extractor(images=images)\n",
    "    examples['pixel_values'] = inputs['pixel_values']\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleType</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tshirts</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shirts</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Casual Shoes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Watches</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sports Shoes</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kurtas</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tops</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Handbags</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heels</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wallets</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    articleType  label_num\n",
       "7       Tshirts          7\n",
       "4        Shirts          4\n",
       "0  Casual Shoes          0\n",
       "9       Watches          9\n",
       "5  Sports Shoes          5\n",
       "3        Kurtas          3\n",
       "6          Tops          6\n",
       "1      Handbags          1\n",
       "2         Heels          2\n",
       "8       Wallets          8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_labels = pd.DataFrame(filtered_df.groupby('articleType').size().reset_index().sort_values(0,ascending = False)[:11]['articleType'])\n",
    "top_labels_list = sorted(list(top_labels['articleType']))\n",
    "top_labels['label_num'] = top_labels['articleType'].apply(lambda x: top_labels_list.index(x))\n",
    "top_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c17e5b253a4f70ac8d7b4561548fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2426ea2922a64d27a1651d64fa69b33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/535 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d915819127b474c877da65809f609f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Features, ClassLabel, Array3D\n",
    "\n",
    "# we need to define the features ourselves as both the img and pixel_values have a 3D shape \n",
    "features = Features({\n",
    "    'label': ClassLabel(names = top_labels_list),\n",
    "    'img': Array3D(dtype=\"int64\", shape=(3,32,32)),\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "})\n",
    "\n",
    "preprocessed_train_ds = train_ds.map(preprocess_images, batched=True, features=features)\n",
    "preprocessed_val_ds = val_ds.map(preprocess_images, batched=True, features=features)\n",
    "preprocessed_test_ds = test_ds.map(preprocess_images, batched=True, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'img', 'pixel_values'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_train_ds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "\n",
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, num_labels=10, dropout_rate=0.1):\n",
    "        super(ViTForImageClassification, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.last_layer = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        # Get the outputs from the ViT model\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "\n",
    "        # Extract the CLS token embedding ([:, 0]) and apply dropout\n",
    "        cls_embedding = self.dropout(outputs.last_hidden_state[:, 0])\n",
    "\n",
    "        # Pass through the classification layer\n",
    "        logits = self.last_layer(cls_embedding)\n",
    "\n",
    "        # Compute loss only if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        # Return a SequenceClassifierOutput object\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "num_epochs = 6\n",
    "batch_size = 4\n",
    "learning_rate = 2e-5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(preprocessed_train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(preprocessed_val_ds, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "metric_name = \"accuracy\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"test-clothing\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    logging_dir='logs',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "model = ViTForImageClassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=preprocessed_train_ds,\n",
    "    eval_dataset=preprocessed_val_ds,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7b394bed454f21a4a9bb4a80e6cd86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/978 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49a0eb2208b4fdabe81a15910063f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9862049221992493, 'eval_accuracy': 0.7716981132075472, 'eval_runtime': 60.5878, 'eval_samples_per_second': 8.748, 'eval_steps_per_second': 2.195, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8fafad4d9041d6b9d838ec22bf63b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7298076152801514, 'eval_accuracy': 0.8509433962264151, 'eval_runtime': 61.119, 'eval_samples_per_second': 8.672, 'eval_steps_per_second': 2.176, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d8e12cfae7455b82646420256e38d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6045047640800476, 'eval_accuracy': 0.879245283018868, 'eval_runtime': 209.3054, 'eval_samples_per_second': 2.532, 'eval_steps_per_second': 0.635, 'epoch': 3.0}\n",
      "{'loss': 0.9145, 'grad_norm': 2.859696388244629, 'learning_rate': 9.775051124744377e-06, 'epoch': 3.07}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d97d9bec3284493ae8c3d084450d570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5406275391578674, 'eval_accuracy': 0.8773584905660378, 'eval_runtime': 211.9499, 'eval_samples_per_second': 2.501, 'eval_steps_per_second': 0.628, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4773eec0864967bf0f85ccc7bb2bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5030139684677124, 'eval_accuracy': 0.8924528301886793, 'eval_runtime': 48.5654, 'eval_samples_per_second': 10.913, 'eval_steps_per_second': 2.739, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b98c067b8f849f5b04a4949dd7b032d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4944866895675659, 'eval_accuracy': 0.8867924528301887, 'eval_runtime': 66.5269, 'eval_samples_per_second': 7.967, 'eval_steps_per_second': 1.999, 'epoch': 6.0}\n",
      "{'train_runtime': 4956.8688, 'train_samples_per_second': 1.968, 'train_steps_per_second': 0.197, 'train_loss': 0.6203218393774365, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=978, training_loss=0.6203218393774365, metrics={'train_runtime': 4956.8688, 'train_samples_per_second': 1.968, 'train_steps_per_second': 0.197, 'total_flos': 0.0, 'train_loss': 0.6203218393774365, 'epoch': 6.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e18ccb3b8024c5a9941f8eea9301004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = trainer.predict(preprocessed_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.42425525188446045, 'test_accuracy': 0.8932584269662921, 'test_runtime': 47.5455, 'test_samples_per_second': 11.231, 'test_steps_per_second': 2.818}\n"
     ]
    }
   ],
   "source": [
    "print(outputs.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\ASUS\\\\Desktop\\\\shoppin-assignment\\\\vit-model-trained\\\\preprocessor_config.json']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), r\"C:\\Users\\ASUS\\Desktop\\shoppin-assignment\\vit-model-trained\\model.pth\")\n",
    "\n",
    "# Save the feature extractor\n",
    "feature_extractor.save_pretrained(r\"C:\\Users\\ASUS\\Desktop\\shoppin-assignment\\vit-model-trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_10692\\4156166988.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"C:\\Users\\ASUS\\Desktop\\shoppin-assignment\\vit-model-trained\\model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (last_layer): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the model first\n",
    "model = ViTForImageClassification(num_labels=10)  # or however many labels you have\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\ASUS\\Desktop\\shoppin-assignment\\vit-model-trained\\model.pth\"))\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_forward(self, pixel_values):\n",
    "    # Get the ViT embeddings\n",
    "    outputs = self.vit(pixel_values)\n",
    "    # Get the pooled output (CLS token)\n",
    "    pooled_output = outputs[1]  # or outputs.pooler_output\n",
    "    return pooled_output\n",
    "\n",
    "# Replace model's forward method temporarily\n",
    "model.forward = modified_forward.__get__(model)\n",
    "\n",
    "# Run inference to get the embeddings\n",
    "embeddings = []\n",
    "for batch in preprocessed_test_ds:\n",
    "    # Get the preprocessed pixel values which are already in the correct format\n",
    "    inputs = batch['pixel_values']\n",
    "    # Convert to tensor and move to device if needed\n",
    "    inputs = torch.tensor(inputs).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(pixel_values=inputs)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "vectors = {test_ids[i]: embeddings[i].tolist() for i in range(len(test_ids))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(vectors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_similarity_matrix(embeddings):\n",
    "    \"\"\"\n",
    "    Compute a pairwise cosine similarity matrix from the embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = np.array(embeddings)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    return similarity_matrix\n",
    "\n",
    "def evaluate_similarity(vectors, labels, top_k=5):\n",
    "    \"\"\"\n",
    "    Evaluate the model using embeddings for image similarity.\n",
    "    \n",
    "    Args:\n",
    "        vectors (dict): A dictionary of image IDs and their corresponding embeddings.\n",
    "        labels (list): Ground truth labels for the images.\n",
    "        top_k (int): Number of top results to consider for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Convert vectors to matrix and maintain IDs\n",
    "    ids = list(vectors.keys())  # Convert keys to list to ensure consistent ordering\n",
    "    embeddings = [vectors[key] for key in ids]  # Use ids list directly\n",
    "    \n",
    "    # Convert embeddings to 2D array if needed\n",
    "    embeddings = np.array(embeddings)\n",
    "    if len(embeddings.shape) == 3:\n",
    "        embeddings = embeddings.squeeze(1)  # Remove extra dimension if present\n",
    "        \n",
    "    similarity_matrix = compute_similarity_matrix(embeddings)\n",
    "\n",
    "    # Evaluation metrics\n",
    "    correct_top_k = 0\n",
    "    total_queries = len(labels)\n",
    "    \n",
    "    # Convert labels to numpy array for easier indexing\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    for idx in tqdm(range(len(ids)), desc=\"Evaluating Precision@K\"):\n",
    "        query_label = labels[idx]\n",
    "        \n",
    "        # Get similarities for current query\n",
    "        similarities = similarity_matrix[idx].copy()  # Make a copy to avoid modifying original\n",
    "        similarities[idx] = -np.inf  # Exclude self-match\n",
    "        \n",
    "        # Get top-k most similar image indices\n",
    "        top_k_indices = np.argsort(similarities)[-top_k:]\n",
    "        \n",
    "        # Check if any of the top-k have the same label\n",
    "        top_k_labels = labels[top_k_indices]\n",
    "        if query_label in top_k_labels:\n",
    "            correct_top_k += 1\n",
    "\n",
    "    precision_at_k = correct_top_k / total_queries\n",
    "\n",
    "    return {\n",
    "        \"precision@K\": precision_at_k,\n",
    "        \"total_queries\": total_queries,\n",
    "        \"correct_top_k\": correct_top_k\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Precision@K: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 17299.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topk=1 Evaluation Results: {'precision@K': 0.8769230769230769, 'total_queries': 520, 'correct_top_k': 456}\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Precision@K: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 20505.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topk=5 Evaluation Results: {'precision@K': 0.9634615384615385, 'total_queries': 520, 'correct_top_k': 501}\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Precision@K: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [00:00<00:00, 21393.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topk=10 Evaluation Results: {'precision@K': 0.9769230769230769, 'total_queries': 520, 'correct_top_k': 508}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "results = evaluate_similarity(vectors, outputs.label_ids, top_k=1)\n",
    "print(\"Topk=1 Evaluation Results:\", results)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "results = evaluate_similarity(vectors, outputs.label_ids, top_k=5)\n",
    "print(\"Topk=5 Evaluation Results:\", results)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "results = evaluate_similarity(vectors, outputs.label_ids, top_k=10)\n",
    "print(\"Topk=10 Evaluation Results:\", results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
